{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BOW) is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  djfjkdfjkjkffdk edited\n",
       "1       Faith has exhibited enthusiasm in taking on th...\n",
       "2       He now has now understood the structure of gra...\n",
       "3       The Intern was oriented on ICT setup and Infra...\n",
       "4       The student was oriented on the organization s...\n",
       "                              ...                        \n",
       "3909    Intern managed to set up a monitoring tool on ...\n",
       "3910    This was impressive work. The intern opened gr...\n",
       "3911    The week went well. Students had a background ...\n",
       "3912    It was best to attach the intern to an actual ...\n",
       "3913    He managed to complete the task as expected. H...\n",
       "Name: Comment, Length: 3914, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Comments.xlsx')\n",
    "df['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import numpy\n",
    "set(stopwords.words('english'))\n",
    "def word_extraction(sentence):   \n",
    "    ignore = ['a', \"the\", \"is\"]    \n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):    \n",
    "    words = []    \n",
    "    for sentence in sentences:        \n",
    "        w = word_extraction(sentence)        \n",
    "        words.extend(w)            \n",
    "        words = sorted(list(set(words)))    \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow(allsentences):        \n",
    "    vocab = tokenize(allsentences)    \n",
    "    print(\"Word List for Document \\n{0} \\n\".format(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsentences = [\"Joe waited for the train train\", \"The train was late\", \"Mary and Samantha took the bus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenize(allsentences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe waited for the train train\n",
      "[0. 1. 0. 0.]\n",
      "\n",
      "Joe waited for the train train\n",
      "[0. 1. 0. 1.]\n",
      "\n",
      "Joe waited for the train train\n",
      "[1. 1. 0. 1.]\n",
      "\n",
      "Joe waited for the train train\n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "Joe waited for the train train\n",
      "[1. 1. 2. 1.]\n",
      "\n",
      "The train was late\n",
      "[0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in allsentences:        \n",
    "    words = word_extraction(sentence)        \n",
    "    bag_vector = numpy.zeros(len(vocab))        \n",
    "    for w in words:            \n",
    "        for i,word in enumerate(vocab):                \n",
    "            if word == w:                     \n",
    "                bag_vector[i] += 1                            \n",
    "                print(\"{0}\\n{1}\\n\".format(sentence,numpy.array(bag_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5904c03de541aee3171dc2d946da9df85831543bc9d72f4542217b15b1550de6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
